1.3M Hopper GPUs 3.6M Blackwell GPUs So Far (2-GPUs per Chip) Top 4 US CSPs 20252024 2026 2027 202820252022 2023 2024 $250 $1,000 $750 $500 NVDA DC Revenue Data Center Capex $1T+ (Dell’Oro Projections) Computing At Inflection Point Aerial-Sionna 6G AI-RAN Omniverse-Cosmos AI Factory Blueprint and Full Stack Omniverse-Cosmos Isaac GR00T DRIVE AV NVIDIA AI Enterprise Leading Technology Full-Stack AI CUDA-X Diverse Applications Rich Developer Ecosystem Announcing NVIDIA Halos Chip-to-Deployment AV Safety System Grace Blackwell in Full Production 18 BlueField DPUs 72 ConnectX-8 NICs 2,592 Grace CPU Cores 18 NVLink Switches 130 TB/s All-to-All 72 Blackwell GPUs 1.1 EF FP4 NVIDIA Blackwell System 130 Trillion Transistors 576 Memory Chips 20 TB 576 TB/s Inference At-Scale is Extreme Computing Tokens per Second For Factory Throughput Tokens per Second For One User Revenue Smart AI Fast Response FLOPS HBM Memory HBM Bandwidth Software and Architecture Prefill Decode Query KV $ Distributed Inference Serving Library Announcing NVIDIA Dynamo Disaggregated Inference GPU Resource Allocation KV Cache Routing Communication Library (NIXL) 0 1,000,000 2,000,000 3,000,000 4,000,000 5,000,000 6,000,000 7,000,000 8,000,000 9,000,000 10,000,000 0 100 200 300 400 500 600 TPS for 1 User Blackwell Giant Leap in Inference Performance TPS / MW Throughput Hopper Smart AI Fast Response 0 1,000,000 2,000,000 3,000,000 4,000,000 5,000,000 6,000,000 7,000,000 8,000,000 9,000,000 10,000,000 0 100 200 300 400 500 600 TPS for 1 User Blackwell Giant Leap in Inference Performance TPS / MW Blackwell NVL8 FP8 Hopper Throughput Smart AI Fast Response 0 1,000,000 2,000,000 3,000,000 4,000,000 5,000,000 6,000,000 7,000,000 8,000,000 9,000,000 10,000,000 0 100 200 300 400 500 600 TPS for 1 User Blackwell Giant Leap in Inference Performance TPS / MW Blackwell NVL8 FP4 Hopper Blackwell NVL8 FP8 Throughput Smart AI Fast Response 0 1,000,000 2,000,000 3,000,000 4,000,000 5,000,000 6,000,000 7,000,000 8,000,000 9,000,000 10,000,000 0 100 200 300 400 500 600 TPS for 1 User Blackwell Giant Leap in Inference Performance TPS / MW Blackwell NVL8 FP4 Blackwell NVL72 FP4 Hopper Blackwell NVL8 FP8 Throughput Smart AI Fast Response 0 1,000,000 2,000,000 3,000,000 4,000,000 5,000,000 6,000,000 7,000,000 8,000,000 9,000,000 10,000,000 0 100 200 300 400 500 600 TPS for 1 User Blackwell Giant Leap in Inference Performance TPS / MW Hopper Dynamo Blackwell NVL8 FP4 Blackwell Dynamo NVL72 FP4 Blackwell NVL72 FP4 Blackwell NVL8 FP8 Throughput Smart AI Fast Response 0 1,000,000 2,000,000 3,000,000 4,000,000 5,000,000 6,000,000 7,000,000 8,000,000 9,000,000 10,000,000 0 100 200 300 400 500 600 TPS for 1 User Blackwell 25X Hopper FP4, NVL72, Dynamo, and TRT-LLM Continuous Optimization 1K ISL / 2K OSL TPS / MW EP32, Batch 1792, Disagg Off EP64, Batch 512, Disagg Off EP64, Batch 384, Disagg Off EP64, Batch 256, Disagg Off EP64, Batch 128, Disagg Off EP64+EP4, Batch 64, 26% Context, MTP On EP64+EP4, Batch 32, 19% Context, MTP On EP64+EP4, Batch 16, 12% Context, MTP On EP64+EP4, Batch 8, 7% Context, MTP On EP64+EP4, Batch 4, 4% Context, MTP On EP64+EP4, Batch 2, 2% Context, MTP On TEP8EP8+EP4, Batch 4, 1% Context, MTP On EP8, Batch 3072, Disagg Off EP64, Batch 896, Disagg Off TEP8+EP4, Batch 2, 1% Context, MTP On TEP16+EP4, Batch 2, 1% Context, MTP On Hopper FP8 NVL8 Dynamo Throughput Blackwell FP4 NVL72 Dynamo Smart AI Fast Response 0 100,000 200,000 300,000 400,000 500,000 600,000 700,000 800,000 900,000 1,000,000 0 100 200 300 400 500 600 Blackwell 40X Hopper FP4, NVL72, Dynamo, and TRT-LLM Continuous Optimization 32K ISL / 8K OSL TPS / MW EP64PP2+PP4, Batch 224, 50% Context EP64PP4+PP4, Batch 224, 46% Context EP64PP4+PP4, Batch 160, 44% Context EP64+EP4, Batch 16, 43% Context, MTP On EP64+EP4, Batch 8, 36% Context, MTP On EP64+EP4, Batch 4, 29% Context, MTP On EP64+EP4, Batch 2, 19% Context, MTP On EP64+EP4, Batch 1, 12% Context, MTP On TEP4+EP4, Batch 1, 3% Context, MTP On TEP8+EP4, Batch 1, 2%Context, MTP On TEP16+EP4, Batch 1, 1% Context, MTP On TEP32+EP4, Batch 1, 1% Context, MTP On EP64PP2+PP4, Batch 480, 52% Context EP64PP2+PP4, Batch 160, 48% Context TPS for 1 User EP64+EP4, Batch 32, 47% Context, MTP On Throughput Hopper FP8 NVL8 Dynamo Blackwell FP4 NVL72 Dynamo Smart AI Fast Response 100 MW AI Factory H100 NVL8 GPU Dies 45K Racks 1,400 Data Center Productivity 300M DeepSeek R1 Context and Generation, , ISL=32K, OSL=8K @ Pareto optimal TPS/User DeepSeek R1 Context and Generation, , ISL=32K, OSL=8K @ Pareto optimal TPS/User 100 MW AI Factory H100 NVL8 GB200 NVL72 GPU Dies 45K 85K Racks 1,400 600 Data Center Productivity 300M 12,000M 40X More Token Revenue Blackwell 40X Hopper Inference Performance NVLink Flops Per Watt ~ AI Factory Output 1.1 EF Dense FP4 Inference 0.36 EF FP8 Training 1.5X GB200 NVL72 Blackwell Ultra NVL72 Second Half 2025 New Attention Instructions 2X 20 TB HBM | 40 TB Fast Memory 1.5X 14.4 TB/s CX8 2X Oberon Rack Liquid Cooled Grace 2 Reticle-Sized GPUs 15PF Dense FP4 | 288GB HBM3e Blackwell Ultra 2 Reticle-Sized GPUs 50PF FP4 | 288GB HBM4 88 Custom Arm Cores 176 Threads 1.8 TB/s NVLink-C2C 3.6 EF FP4 Inference 1.2 EF FP8 Training 3.3X GB300 NVL72 Vera Rubin NVL144 Second Half 2026 13 TB/s HBM4 75 TB Fast Memory 1.6X 260 TB/s NVLink6 2X 28.8 TB/s CX9 2X Oberon Rack Liquid Cooled Vera Rubin 4 Reticle-Sized GPUs 100PF FP4 | 1TB HBM4e 15 EF FP4 Inference 5 EF FP8 Training 14X GB300 NVL72 Rubin Ultra NVL576 Second Half 2027 4.6 PB/s HBM4e 365 TB Fast Memory 8X Kyber Rack Liquid Cooled Vera Rubin Ultra 88 Custom Arm Cores 176 Threads 1.8 TB/s NVLink-C2C 1.5 PBs NVLink7 12X 115.2 TB/s CX9 8X NVIDIA Rubin System Vera Rubin NVLink576 72 BlueField DPUs 576 ConnectX-9 NICs 12,672 Vera CPU Cores 144 NVLink Switches 1500 PB/s 576 Rubin GPUs 15 EF FP4 1,300 Trillion Transistors 2,304 Memory Chips 150 TB 4600 PB/s NVIDIA Blackwell System 18 BlueField DPUs 72 ConnectX-8 NICs 2,592 Grace CPU Cores 18 NVLink Switches 130 TB/s 72 Blackwell GPUs 1.1 EF FP4 130 Trillion Transistors 576 Memory Chips 20 TB 576 TB/s 1 68 900 Hopper Blackwell Rubin 1 0.13 0.03 Hopper Blackwell Rubin *Without sparsity Perf = Leadership TCO / Perf = Cost Fast Roadmap Drives AI Factory Economics Spectrum-X “Supercharged” Ethernet Powering the World’s Most Advanced AI Clouds and Factories Spectrum-X Ethernet SwitchSpectrum-X SuperNIC NVIDIA Photonics CPO Co-Invention With Ecosystem Partners 1st 1.6T Silicon Photonics CPO Chip - New Micro Ring Modulators (MRM) 1st 3D-Stacked Silicon Photonics Engine with TSMC Process High-Power, High-Efficiency Lasers Detachable Fiber Connectors 100’s of Patents, Licensed to Partners Quantum-X Integrated Silicon Photonics 2nd Half 2025 Spectrum-X Integrated Silicon Photonics 2nd Half 2026 Photonic IC Co-Packaged Optics Quantum-X Photonic Switch Electronic IC Fiber Connector 3D Stacked Electronic and Photonic ICs Laser Source Package External Laser Source Module Interposer Optical Sub-Assembly COUPE uLens with surface coupling Announcing NVIDIA Photonics Switch Systems World’s Most Advanced Co-Packaged Optical Switches 144 Ports of 800G | 576 x 200G 128 Ports of 800G | 512 x 200G 512 Ports of 800G | 2K x 200G Spectrum-X Photonics 2nd Half 2026 Quantum-X Photonics 2nd Half 2025 2025 2026 2027 2028 COMPUTE Blackwell 8S HBM3e Blackwell Ultra 8S HBM3e Spectrum7 204T, CPO CX10 Feynman Next–Gen HBM Vera CPU Rubin 8S HBM4 Rubin Ultra 16S HBM4e Oberon NVL72 Liquid Cooled Kyber NVL576 Liquid Cooled NVIDIA Paves Road to Gigawatt AI Factories One-Year Rhythm | Full-Stack | One Architecture | CUDA Everywhere Blackwell Feynman NVLINK (SCALE-UP) NETWORKING (SCALE-OUT) Grace CPU 5th Gen NVL 72 1800 GB/s Spectrum5 51T CX8 800G Spectrum6 102T, CPO CX9 1600G 6th Gen NVSwitch 3600 GB/s Vera CPU 7th Gen NVSwitch 3600 GB/s SYSTEM 8th Gen NVSwitch NVL-Next Rubin Reinventing $500B Enterprise IT For the Age of AI Compute Networking Storage AI OS INFRASTRUCTURE DGX Station The Ultimate Workstation for AI and Data Science GB300 Superchip 784 GB Unified System Memory 20,000 AI TFLOPS ConnectX-8 SuperNIC DGX Spark DGX Station RTX PRO Enterprise Server DGX B200 DGX GB300 RTX PRO Workstation 1 PFLOPS 20 PFLOPS 15 EFLOPS NVIDIA AI Infrastructure for Enterprise Computing NeMo Retriever Vector Database NVIDIA cuVS Enterprise Files Announcing Storage Leaders Build AI Data Platforms for Enterprise AI Semantic Query Agent NVIDIA AI-Q Blueprint NVIDIA Spectrum-X NVIDIA BlueField / ConnectX Storage Node NVIDIA Blackwell NVIDIA BlueField Compute Node Llama Nemotron Announcing NVIDIA Llama Nemotron Reasoning Distilled, Quantized, Aligned, and Optimized by NVIDIA 0 1,000 2,000 3,000 50 55 60 65 70 75 80 Tokens/s Average Accuracy Across Agentic Tasks (%) NVIDIA Llama Nemotron Super 49B DeepSeek R1 Llama 70B Llama 3.3 70B Nano Ultra Super Internet Video Pre-Training Human Demonstration Post-Training Synthetic Data Generation and RL in Physics Sim Cosmos NVIDIA Omniverse With Cosmos Physical AI Digital Twin Operating System NVIDIA Omniverse Announcing NVIDIA Isaac GR00T N1 Humanoid Foundation Model​ Pick up the industrial object and place in yellow bin. Sensor Tokens Text Tokens Vision Language Model Diffusion Transformer Model Action Tokens BLACKWELL IN FULL PRODUCTION $1T Computing Inflection Point BLACKWELL NVL72 AND DYNAMO 40X FOR INFERENCE Reasoning 100X One-Shot Blackwell 40X Hopper VERA RUBIN NVIDIA PHOTONICS AI INFRASTRUCTURE FOR AI CLOUDS Annual Rhythm for the World to Build-Out AI Infrastructure AI INFRASTRUCTURE FOR ROBOTS Physical AI For $50T Industrial and Robotics AI INFRASTRUCTURE FOR $500B ENTERPRISE IT New Compute, Networking, Storage, Software NVIDIA GTC 2025​